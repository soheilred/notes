@article{MEHTA2007237,
title = "Optimal detection and control strategies for invasive species management",
journal = "Ecological Economics",
volume = "61",
number = "2",
pages = "237 - 245",
year = "2007",
issn = "0921-8009",
doi = "https://doi.org/10.1016/j.ecolecon.2006.10.024",
url = "http://www.sciencedirect.com/science/article/pii/S0921800906005568",
author = "Shefali V. Mehta and Robert G. Haight and Frances R. Homans and Stephen Polasky and Robert C. Venette",
keywords = "Invasive species, Non-native, Detection, Risk management"
}

@article{ecs2.1263,
author = {Meisner, Matthew H. and Rosenheim, Jay A. and Tagkopoulos, Ilias},
title = {A data-driven, machine learning framework for optimal pest management in cotton},
journal = {Ecosphere},
year = "2016",
volume = {7},
number = {3},
pages = {e01263},
keywords = {agricultural informatics, applied pest management, Markov decision processes, optimal pest management},
doi = {10.1002/ecs2.1263},
url = {https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/ecs2.1263},
eprint = {https://esajournals.onlinelibrary.wiley.com/doi/pdf/10.1002/ecs2.1263},
abstract = {Abstract Despite the significant effects of agricultural pest management on crop yield, profit, environmental quality, and sustainability, farmers oftentimes lack data-driven decision support to help optimize pest management strategies. To address this need, we curated a comprehensive data set that consists of pest, pest management, and yield information from 1498 commercial cotton crops in California's San Joaquin Valley between 1997 and 2008. Using this data set, we built a Markov decision process model to identify the optimal management policy of a key cotton pest, Lygus hesperus, that balances the tradeoff between yield loss and the cost of pesticide applications. Our results show that pesticide applications targeting L. hesperus are only economically optimal during the first 2 weeks of June, and pesticide applications were associated with increased risk of an unprofitable harvest. About 46\% of the observations in our data set involved at least one pesticide application outside of this optimal window, demonstrating the need for a data-driven approach to crop management. Sensitivity analyses on parameter perturbations and reduced data set sizes suggest that our methodology provides a robust policy-making tool, even in noisy data sets.}
}

@article{taleghan15a,
  author  = {Majid Alkaee Taleghan and Thomas G. Dietterich and Mark Crowley and Kim Hall and H. Jo Albers},
  title   = {PAC Optimal MDP Planning with Application to Invasive Species Management},
  journal = {Journal of Machine Learning Research},
  year    = {2015},
  volume  = {16},
  pages   = {3877-3903},
  url     = {http://jmlr.org/papers/v16/taleghan15a.html}
}

@Article{Hall2018,
author="Hall, Kim Meyer
and Albers, Heidi J.
and Alkaee Taleghan, Majid
and Dietterich, Thomas G.",
title="Optimal Spatial-Dynamic Management of Stochastic Species Invasions",
journal="Environmental and Resource Economics",
year="2018",
month="Jun",
day="01",
volume="70",
number="2",
pages="403--427",
abstract="Recent analyses demonstrate that the spatial--temporal behavior of invasive species requires optimal management decisions over space and time. From a spatial perspective, this bioeconomic optimization model broadens away from invasive species spread at a frontier or to neighbors by examining short and long-distance dispersal, directionality in spread, and network geometry. In terms of uncertainty and dynamics, this framework incorporates several sources of stochasticity, decisions with multi-year implications, and temporal ecological processes. This paper employs a unique Markov decision process planning algorithm and a Monte Carlo simulation of the stochastic system to explore the spatial-dynamic optimal policy for a river network facing a bioinvasion, with Tamarisk as an example. In addition to exploring the spatial, stochastic, and dynamic aspects of management of invasions, the results demonstrate how the interaction of spatial and multi-period processes contributes to finding the optimal policy. Those interactions prove critical in determining the right management tool, in the right location, at the right time, which informs the management implications drawn from simpler frameworks. In particular, as compared to other modeling framework's policy prescriptions, the framework here finds more use of the management tool restoration and more management in highly connected locations, which leads to a less invaded system over time.",
issn="1573-1502",
doi="10.1007/s10640-017-0127-6",
url="https://doi.org/10.1007/s10640-017-0127-6"
}

@article{ALBERS201844,
title = "The Role of Restoration and Key Ecological Invasion Mechanisms in Optimal Spatial-Dynamic Management of Invasive Species",
journal = "Ecological Economics",
volume = "151",
pages = "44 - 54",
year = "2018",
issn = "0921-8009",
doi = "https://doi.org/10.1016/j.ecolecon.2018.03.031",
url = "http://www.sciencedirect.com/science/article/pii/S0921800917310455",
author = "Heidi J. Albers and Kim Meyer Hall and Katherine D. Lee and Majid Alkaee Taleghan and Thomas G. Dietterich",
keywords = "Spatial, Dynamic optimization, Meta-population, Dispersal, Species interactions, Species competition, Bio-invasions, Invasive species, Ecology, River network, Restoration, Habitat, Riparian"
}

@article{doi:10.1111,
author = {Nicol, Sam and Sabbadin, Regis and Peyrard, Nathalie and Chadès, Iadine},
title = {Finding the best management policy to eradicate invasive species from spatial ecological networks with simultaneous actions},
journal = {Journal of Applied Ecology},
volume = {54},
number = {6},
pages = {1989-1999},
keywords = {classification trees, computational sustainability, conservation, Gambusia, invasive species, Markov decision processes, network, optimal control, optimisation, Scaturiginichthys vermeilipinnis},
doi = {10.1111/1365-2664.12884},
url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/1365-2664.12884},
eprint = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/1365-2664.12884},
abstract = {Summary Spatial management of invasive species is more likely to be successful when multiple locations are treated simultaneously. However, selecting the best locations to act is difficult due to the many options available at any time. We design a near-optimal policy for applying multiple actions simultaneously for faster invasive species control within a network. Our method uses a recent optimisation tool, the graph-based Markov decision process (GMDP). Since the policy can be difficult to interpret, we extracted a simpler policy using classification trees. We applied our approach to the eradication of invasive mosquitofish Gambusia holbrooki from the habitat of the red-finned blue-eye Scaturiginichthys vermeilipinnis, a critically endangered fish with a global population that is restricted to seven artesian springs in Queensland, Australia. The policy returned by the GMDP was to manage springs occupied by mosquitofish and their connected neighbours, unless the neighbours were occupied by red-finned blue-eyes. Simultaneous management resulted in rapid declines in simulated mosquitofish occupancy even if eradication effectiveness was low; however, the cost of simultaneous eradication was high and sustained eradication effort was necessary to maintain low mosquitofish occupancy. Synthesis and applications. Our paper finds a near-optimal, multi-action control policy to remove an invasive species from a multi-species spatial network. We introduce the graph-based Markov decision process and apply it to a real case study – eradication of invasive mosquitofish from the habitat of the red-finned blue-eye. We find that the graph-based Markov decision process can generate policies for networks with extremely large state spaces; however, it works best when nodes have fewer than five neighbours. We conclude that simultaneous eradications are effective for rapid control of invasive species; however, managers should consider the cost and time required for an effective eradication program.}
}

@article{DeFarias2003,
abstract = {The curse of dimensionality gives rise to prohibitive computational requirements that render infeasible the exact solution of large-scale stochastic control problems. We study an efficient method based on linear programming for approximating solutions to such problems. The approach "fits" a linear combination of pre-selected basis functions to the dynamic programming cost-to-go function. We develop error bounds that offer performance guarantees and also guide the selection of both basis functions and "state-relevance weights" that influence quality of the approximation. Experimental results in the domain of queueing network control provide empirical support for the methodology.},
author = {de Farias, D. P. and {Van Roy}, B.},
doi = {10.1287/opre.51.6.850.24925},
file = {:home/soheil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/de Farias, Van Roy - 2003 - The Linear Programming Approach to Approximate Dynamic Programming.pdf:pdf},
isbn = {0030-364X},
issn = {0030-364X},
journal = {Operations Research},
mendeley-groups = {RL},
number = {6},
pages = {850--865},
title = {{The Linear Programming Approach to Approximate Dynamic Programming}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.51.6.850.24925},
volume = {51},
year = {2003}
}


@article{Regan2012,
abstract = {The specification of aMarkov decision process (MDP) can be difficult. Reward function specification is especially problematic; in practice, it is often cognitively complex and time-consuming for users to precisely specify rewards. This work casts the problem of specifying rewards as one of preference elicitation and aims to minimize the degree of precision with which a reward function must be specified while still allowing optimal or near-optimal policies to be produced. We first discuss how robust policies can be computed for MDPs given only partial reward information using the minimax regret criterion. We then demonstrate how regret can be reduced by efficiently eliciting reward information using bound queries, using regret-reduction as a means for choosing suitable queries. Empirical results demonstrate that regret-based reward elicitation offers an effective way to produce near-optimal policies without resorting to the precise specification of the entire reward function.},
archivePrefix = {arXiv},
arxivId = {1205.2619},
author = {Regan, Kevin and Boutilier, Craig},
eprint = {1205.2619},
file = {:home/soheil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Regan, Boutilier - 2012 - Regret-based Reward Elicitation for Markov Decision Processes.pdf:pdf},
mendeley-groups = {RL},
pages = {444--451},
title = {{Regret-based Reward Elicitation for Markov Decision Processes}},
url = {http://arxiv.org/abs/1205.2619},
year = {2012}
}


@article{Petrik2010a,
abstract = {Approximate dynamic programming has been used successfully in a large variety of domains, but it relies on a small set of provided approximation features to calculate solutions reliably. Large and rich sets of features can cause existing algorithms to overfit because of a limited number of samples. We address this shortcoming using {\$}L{\_}1{\$} regularization in approximate linear programming. Because the proposed method can automatically select the appropriate richness of features, its performance does not degrade with an increasing number of features. These results rely on new and stronger sampling bounds for regularized approximate linear programs. We also propose a computationally efficient homotopy method. The empirical evaluation of the approach shows that the proposed method performs well on simple MDPs and standard benchmark problems.},
archivePrefix = {arXiv},
arxivId = {1005.1860},
author = {Petrik, Marek and Taylor, Gavin and Parr, Ron and Zilberstein, Shlomo},
eprint = {1005.1860},
file = {:home/soheil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Petrik et al. - 2010 - Feature Selection Using Regularization in Approximate Linear Programs for Markov Decision Processes(2).pdf:pdf},
title = {{Feature Selection Using Regularization in Approximate Linear Programs for Markov Decision Processes}},
url = {http://arxiv.org/abs/1005.1860},
year = {2010}
}


@article{VanRoy2002,
author = {Daniela P. de Farias, Benjamin Van Roy},
file = {:home/soheil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Roy - 2002 - Approximate Dynamic Programming via Linear Programming(2).pdf:pdf},
journal = {Comparative and General Pharmacology},
keywords = {kernel regression,linear programming,support vector machines},
number = {1995},
pages = {255--269},
title = {{Approximate Dynamic Programming via Linear Programming}},
year = {2002}
}

@article{VanRoy2006c,
author = {{Van Roy}, Benjamin and Benjamin},
doi = {10.1287/moor.1060.0188},
file = {:home/soheil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Roy - 2006 - Performance Loss Bounds for Approximate Value Iteration with State Aggregation(2).pdf:pdf},
issn = {0364-765X},
journal = {Mathematics of Operations Research},
keywords = {approximate value iteration,state aggregation,temporal-difference learning},
mendeley-groups = {RL},
month = {may},
number = {2},
pages = {234--244},
publisher = {INFORMS},
title = {{Performance Loss Bounds for Approximate Value Iteration with State Aggregation}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/moor.1060.0188},
volume = {31},
year = {2006}
}

@article{DeFarias2003,
abstract = {The curse of dimensionality gives rise to prohibitive computational requirements that render infeasible the exact solution of large-scale stochastic control problems. We study an efficient method based on linear programming for approximating solutions to such problems. The approach "fits" a linear combination of pre-selected basis functions to the dynamic programming cost-to-go function. We develop error bounds that offer performance guarantees and also guide the selection of both basis functions and "state-relevance weights" that influence quality of the approximation. Experimental results in the domain of queueing network control provide empirical support for the methodology.},
author = {de Farias, D. P. and {Van Roy}, B.},
doi = {10.1287/opre.51.6.850.24925},
file = {:home/soheil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/de Farias, Van Roy - 2003 - The Linear Programming Approach to Approximate Dynamic Programming.pdf:pdf},
isbn = {0030-364X},
issn = {0030-364X},
journal = {Operations Research},
mendeley-groups = {RL},
number = {6},
pages = {850--865},
title = {{The Linear Programming Approach to Approximate Dynamic Programming}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.51.6.850.24925},
volume = {51},
year = {2003}
}

@article{Huang2018,
abstract = {We propose a framework for ensuring safe behavior of a reinforcement learning agent when the reward function may be difficult to specify. In order to do this, we rely on the existence of demonstrations from expert policies, and we provide a theoretical framework for the agent to optimize in the space of rewards consistent with its existing knowledge. We propose two methods to solve the resulting optimization: an exact ellipsoid-based method and a method in the spirit of the "follow-the-perturbed-leader" algorithm. Our experiments demonstrate the behavior of our algorithm in both discrete and continuous problems. The trained agent safely avoids states with potential negative effects while imitating the behavior of the expert in the other states.},
archivePrefix = {arXiv},
arxivId = {1805.08313},
author = {Huang, Jessie and Wu, Fa and Precup, Doina and Cai, Yang},
eprint = {1805.08313},
file = {:home/soheil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 2018 - Learning Safe Policies with Expert Guidance.pdf:pdf},
number = {Theorem 1},
pages = {1--17},
title = {{Learning Safe Policies with Expert Guidance}},
url = {http://arxiv.org/abs/1805.08313},
year = {2018}
}

@article{abbeel2004,
abstract = {MANUAL FOR SORVALL LEGEND MICRO 21R},
author = {{Pieter Abbeel} and {Andrew Y.}},
doi = {10.1145/1015330.1015430},
file = {:home/soheil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Microcentrifuge - 2011 - Apprenticeship Learning via Inverse Reinforcement Learning.pdf:pdf},
isbn = {1581138285},
mendeley-groups = {RL},
number = {346},
pages = {1--2},
title = {{Apprenticeship Learning via Inverse Reinforcement Learning}},
year = {2004}
}

@article{Decision2005c,
author = {PUTERMAN},
file = {:home/soheil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/PUTERMAN - 2005 - Chapter 6 Discounted Markov Decision Problems.pdf:pdf},
mendeley-groups = {RL},
title = {{Markov Decision Processes Discrete Stochastic Dynamic Programming Chapter 6: Discounted Markov Decision Problems}},
year = {1994}
}

@article{alg_for_irl_Ng,
author = {Ng, Andrew y. and {Stuart Russell}},
file = {:home/soheil/Dropbox/unh/re{\_}learning/documents/icml00-irl.pdf:pdf},
title = {algorithms for inverse reinforcement learning},
url = {https://ai.stanford.edu/{~}ang/papers/icml00-irl.pdf}
}


@article{Chow2015,
abstract = {In this paper we address the problem of decision making within a Markov decision process (MDP) framework where risk and modeling errors are taken into account. Our approach is to minimize a risk-sensitive conditional-value-at-risk (CVaR) objective, as opposed to a standard risk-neutral expectation. We refer to such problem as CVaR MDP. Our first contribution is to show that a CVaR objective, besides capturing risk sensitivity, has an alternative interpretation as expected cost under worst-case modeling errors, for a given error budget. This result, which is of independent interest, motivates CVaR MDPs as a unifying framework for risk-sensitive and robust decision making. Our second contribution is to present an approximate value-iteration algorithm for CVaR MDPs and analyze its convergence rate. To our knowledge, this is the first solution algorithm for CVaR MDPs that enjoys error guarantees. Finally, we present results from numerical experiments that corroborate our theoretical findings and show the practicality of our approach.},
archivePrefix = {arXiv},
arxivId = {1506.02188},
author = {Chow, Yinlam and Tamar, Aviv and Mannor, Shie and Pavone, Marco},
eprint = {1506.02188},
file = {:home/soheil/Dropbox/unh/re{\_}learning/documents/Chow.Tamar.Mannor.Pavone.NIPS15.pdf:pdf},
keywords = {reward elicitation,risk,robust mdp},
mendeley-tags = {reward elicitation,robust mdp,risk},
month = {jun},
pages = {1--21},
title = {{Risk-Sensitive and Robust Decision-Making: a CVaR Optimization Approach}},
url = {http://arxiv.org/abs/1506.02188},
year = {2015}
}


@article{DeepakRamachandran2007,
abstract = {Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elicitation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert's actions to derive a probability distribution over the space of reward functions. We present efficient algorithms that find solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches.},
archivePrefix = {arXiv},
arxivId = {1206.5264},
author = {{Deepak Ramachandran} and {Eyal Amir}},
doi = {10.1109/72.788640},
eprint = {1206.5264},
file = {:home/soheil/Dropbox/unh/re{\_}learning/documents/IJCAI07-416.pdf:pdf},
isbn = {978-1-4673-2755-8},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Markov-decision processes,action understanding,bayesian,inference,markov decision processes,reinforcement learning,theory mind},
number = {5},
pages = {2586--2591},
pmid = {19574462},
title = {{Bayesian Inverse Reinforcement Learning}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Goal+Inference+as+Inverse+Planning{\#}0},
volume = {10},
year = {2007}
}
