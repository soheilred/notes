@article{Bradtke1996a,
abstract = {Abstract. We introduce two new temporal difference (TD) algorithms based on the theory of linear leastsquares function approximation. We define an algorithm we call Least-Squares TD (LS TD) for which we prove probability-one convergence when it is used with a function approximator linear in the adjustable parameters. We then define a recursive version of this algorithm, Recursive Least-Squares TD (RLS TD). Although these new TD algorithms require more computation per time-step than do Sutton's TD(A) algorithms, they are more efficient in a statistical sense because they extract more information from training experiences. We describe a simulation experiment showing the substantial improvement in learning rate achieved by RLS TD in an example Markov prediction problem. To quantify this improvement, we introduce the TD error variance of a Markov chain, arc,, and experimentally conclude that the convergence rate of a TD algorithm depends linearly on {\~{}}ro. In addition to converging more rapidly, LS TD and RLS TD do not have control parameters, such as a learning rate parameter, thus eliminating the possibility of achieving poor performance by an unlucky choice of parameters.},
author = {Bradtke, Steven J.},
doi = {10.1007/BF00114723},
file = {:home/soheil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bradtke - 1996 - Linear least-squares algorithms for temporal difference learning.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Least-squares,Markov decision problems,Reinforcement learning,Temporal difference methods},
number = {1-3},
pages = {33--57},
title = {{Linear least-squares algorithms for temporal difference learning}},
volume = {22},
year = {1996}
}
