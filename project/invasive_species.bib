@article{MEHTA2007237,
title = "Optimal detection and control strategies for invasive species management",
journal = "Ecological Economics",
volume = "61",
number = "2",
pages = "237 - 245",
year = "2007",
issn = "0921-8009",
doi = "https://doi.org/10.1016/j.ecolecon.2006.10.024",
url = "http://www.sciencedirect.com/science/article/pii/S0921800906005568",
author = "Shefali V. Mehta and Robert G. Haight and Frances R. Homans and Stephen Polasky and Robert C. Venette",
keywords = "Invasive species, Non-native, Detection, Risk management"
}

@article{ecs2.1263,
author = {Meisner, Matthew H. and Rosenheim, Jay A. and Tagkopoulos, Ilias},
title = {A data-driven, machine learning framework for optimal pest management in cotton},
journal = {Ecosphere},
year = "2016",
volume = {7},
number = {3},
pages = {e01263},
keywords = {agricultural informatics, applied pest management, Markov decision processes, optimal pest management},
doi = {10.1002/ecs2.1263},
url = {https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/ecs2.1263},
eprint = {https://esajournals.onlinelibrary.wiley.com/doi/pdf/10.1002/ecs2.1263},
abstract = {Abstract Despite the significant effects of agricultural pest management on crop yield, profit, environmental quality, and sustainability, farmers oftentimes lack data-driven decision support to help optimize pest management strategies. To address this need, we curated a comprehensive data set that consists of pest, pest management, and yield information from 1498 commercial cotton crops in California's San Joaquin Valley between 1997 and 2008. Using this data set, we built a Markov decision process model to identify the optimal management policy of a key cotton pest, Lygus hesperus, that balances the tradeoff between yield loss and the cost of pesticide applications. Our results show that pesticide applications targeting L. hesperus are only economically optimal during the first 2 weeks of June, and pesticide applications were associated with increased risk of an unprofitable harvest. About 46\% of the observations in our data set involved at least one pesticide application outside of this optimal window, demonstrating the need for a data-driven approach to crop management. Sensitivity analyses on parameter perturbations and reduced data set sizes suggest that our methodology provides a robust policy-making tool, even in noisy data sets.}
}

@article{taleghan15a,
  author  = {Majid Alkaee Taleghan and Thomas G. Dietterich and Mark Crowley and Kim Hall and H. Jo Albers},
  title   = {PAC Optimal MDP Planning with Application to Invasive Species Management},
  journal = {Journal of Machine Learning Research},
  year    = {2015},
  volume  = {16},
  pages   = {3877-3903},
  url     = {http://jmlr.org/papers/v16/taleghan15a.html}
}

@Article{Hall2018,
author="Hall, Kim Meyer
and Albers, Heidi J.
and Alkaee Taleghan, Majid
and Dietterich, Thomas G.",
title="Optimal Spatial-Dynamic Management of Stochastic Species Invasions",
journal="Environmental and Resource Economics",
year="2018",
month="Jun",
day="01",
volume="70",
number="2",
pages="403--427",
abstract="Recent analyses demonstrate that the spatial--temporal behavior of invasive species requires optimal management decisions over space and time. From a spatial perspective, this bioeconomic optimization model broadens away from invasive species spread at a frontier or to neighbors by examining short and long-distance dispersal, directionality in spread, and network geometry. In terms of uncertainty and dynamics, this framework incorporates several sources of stochasticity, decisions with multi-year implications, and temporal ecological processes. This paper employs a unique Markov decision process planning algorithm and a Monte Carlo simulation of the stochastic system to explore the spatial-dynamic optimal policy for a river network facing a bioinvasion, with Tamarisk as an example. In addition to exploring the spatial, stochastic, and dynamic aspects of management of invasions, the results demonstrate how the interaction of spatial and multi-period processes contributes to finding the optimal policy. Those interactions prove critical in determining the right management tool, in the right location, at the right time, which informs the management implications drawn from simpler frameworks. In particular, as compared to other modeling framework's policy prescriptions, the framework here finds more use of the management tool restoration and more management in highly connected locations, which leads to a less invaded system over time.",
issn="1573-1502",
doi="10.1007/s10640-017-0127-6",
url="https://doi.org/10.1007/s10640-017-0127-6"
}

@article{ALBERS201844,
title = "The Role of Restoration and Key Ecological Invasion Mechanisms in Optimal Spatial-Dynamic Management of Invasive Species",
journal = "Ecological Economics",
volume = "151",
pages = "44 - 54",
year = "2018",
issn = "0921-8009",
doi = "https://doi.org/10.1016/j.ecolecon.2018.03.031",
url = "http://www.sciencedirect.com/science/article/pii/S0921800917310455",
author = "Heidi J. Albers and Kim Meyer Hall and Katherine D. Lee and Majid Alkaee Taleghan and Thomas G. Dietterich",
keywords = "Spatial, Dynamic optimization, Meta-population, Dispersal, Species interactions, Species competition, Bio-invasions, Invasive species, Ecology, River network, Restoration, Habitat, Riparian"
}

@article{doi:10.1111,
author = {Nicol, Sam and Sabbadin, Regis and Peyrard, Nathalie and Chadès, Iadine},
title = {Finding the best management policy to eradicate invasive species from spatial ecological networks with simultaneous actions},
journal = {Journal of Applied Ecology},
volume = {54},
number = {6},
pages = {1989-1999},
keywords = {classification trees, computational sustainability, conservation, Gambusia, invasive species, Markov decision processes, network, optimal control, optimisation, Scaturiginichthys vermeilipinnis},
doi = {10.1111/1365-2664.12884},
url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/1365-2664.12884},
eprint = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/1365-2664.12884},
abstract = {Summary Spatial management of invasive species is more likely to be successful when multiple locations are treated simultaneously. However, selecting the best locations to act is difficult due to the many options available at any time. We design a near-optimal policy for applying multiple actions simultaneously for faster invasive species control within a network. Our method uses a recent optimisation tool, the graph-based Markov decision process (GMDP). Since the policy can be difficult to interpret, we extracted a simpler policy using classification trees. We applied our approach to the eradication of invasive mosquitofish Gambusia holbrooki from the habitat of the red-finned blue-eye Scaturiginichthys vermeilipinnis, a critically endangered fish with a global population that is restricted to seven artesian springs in Queensland, Australia. The policy returned by the GMDP was to manage springs occupied by mosquitofish and their connected neighbours, unless the neighbours were occupied by red-finned blue-eyes. Simultaneous management resulted in rapid declines in simulated mosquitofish occupancy even if eradication effectiveness was low; however, the cost of simultaneous eradication was high and sustained eradication effort was necessary to maintain low mosquitofish occupancy. Synthesis and applications. Our paper finds a near-optimal, multi-action control policy to remove an invasive species from a multi-species spatial network. We introduce the graph-based Markov decision process and apply it to a real case study – eradication of invasive mosquitofish from the habitat of the red-finned blue-eye. We find that the graph-based Markov decision process can generate policies for networks with extremely large state spaces; however, it works best when nodes have fewer than five neighbours. We conclude that simultaneous eradications are effective for rapid control of invasive species; however, managers should consider the cost and time required for an effective eradication program.}
}

@article{Lagoudakis2004,
abstract = {We propose a new approach to reinforcement learning for control problems which com- bines value-function approximation with linear architectures and approximate policy iter- ation. This new approach is motivated by the least-squares temporal-difference learning algorithm (LSTD) for prediction problems, which is known for its efficient use of sample experiences compared to pure temporal-difference algorithms. Heretofore, LSTD has not had a straightforward application to control problems mainly because LSTD learns the state value function of a fixed policy which cannot be used for action selection and control without a model of the underlying process. Our new algorithm, least-squares policy itera- tion (LSPI), learns the state-action value function which allows for action selection without a model and for incremental policy improvement within a policy-iteration framework. LSPI is a model-free, off-policy method which can use efficiently (and reuse in each iteration) sample experiences collected in any manner. By separating the sample collection method, the choice of the linear approximation architecture, and the solution method, LSPI allows for focused attention on the distinct elements that contribute to practical reinforcement learning. LSPI is tested on the simple task of balancing an inverted pendulum and the harder task of balancing and riding a bicycle to a target location. In both cases, LSPI learns to control the pendulum or the bicycle by merely observing a relatively small number of trials where actions are selected randomly. LSPI is also compared against Q-learning (both with and without experience replay) using the same value function architecture. While LSPI achieves good performance fairly consistently on the difficult bicycle task, Q-learning variants were rarely able to balance for more than a small fraction of the time needed to reach the target location.},
author = {Lagoudakis, Michail G. and Parr, Ronald},
doi = {10.1162/1532443041827907},
file = {:home/soheil/Dropbox/fall2018/re{\_}learning/documents/lagoudakis03a.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Approximate Policy iteration,Least-Squares Methods,Markov Decision Processes,Reinforcement Learning,Value-Function Approximation},
number = {6},
pages = {1107--1149},
title = {{Least-squares policy iteration}},
volume = {4},
year = {2004}
}

@book{Sutton:1998,
 author = {Sutton, Richard S. and Barto, Andrew G.},
 title = {Introduction to Reinforcement Learning},
 year = {1998},
 isbn = {0262193981},
 edition = {1st},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@article{Bradtke1996a,
abstract = {Abstract. We introduce two new temporal difference (TD) algorithms based on the theory of linear leastsquares function approximation. We define an algorithm we call Least-Squares TD (LS TD) for which we prove probability-one convergence when it is used with a function approximator linear in the adjustable parameters. We then define a recursive version of this algorithm, Recursive Least-Squares TD (RLS TD). Although these new TD algorithms require more computation per time-step than do Sutton's TD(A) algorithms, they are more efficient in a statistical sense because they extract more information from training experiences. We describe a simulation experiment showing the substantial improvement in learning rate achieved by RLS TD in an example Markov prediction problem. To quantify this improvement, we introduce the TD error variance of a Markov chain, arc,, and experimentally conclude that the convergence rate of a TD algorithm depends linearly on {\~{}}ro. In addition to converging more rapidly, LS TD and RLS TD do not have control parameters, such as a learning rate parameter, thus eliminating the possibility of achieving poor performance by an unlucky choice of parameters.},
author = {Bradtke, Steven J.},
doi = {10.1007/BF00114723},
file = {:home/soheil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bradtke - 1996 - Linear least-squares algorithms for temporal difference learning.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Least-squares,Markov decision problems,Reinforcement learning,Temporal difference methods},
number = {1-3},
pages = {33--57},
title = {{Linear least-squares algorithms for temporal difference learning}},
volume = {22},
year = {1996}
}

@Article{Sutton1988,
author="Sutton, Richard S.",
title="Learning to predict by the methods of temporal differences",
journal="Machine Learning",
year="1988",
month="Aug",
day="01",
volume="3",
number="1",
pages="9--44",
abstract="This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.",
issn="1573-0565",
doi="10.1007/BF00115009",
url="https://doi.org/10.1007/BF00115009"
}

@book{howard:dp,
  added-at = {2008-03-11T14:52:34.000+0100},
  address = {Cambridge, MA},
  author = {Howard, R. A.},
  biburl = {https://www.bibsonomy.org/bibtex/28b55f737ee6dd7800ffc7952a33bb6bd/idsia},
  citeulike-article-id = {2380352},
  interhash = {7eed9f4f6bd1f9ee063d80d0f732e48f},
  intrahash = {8b55f737ee6dd7800ffc7952a33bb6bd},
  keywords = {inaki},
  priority = {2},
  publisher = {MIT Press},
  timestamp = {2008-03-11T14:56:12.000+0100},
  title = {Dynamic Programming and Markov Processes},
  year = 1960
}


